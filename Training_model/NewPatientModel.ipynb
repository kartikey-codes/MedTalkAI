{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset Generation** to train Spacy Model"
      ],
      "metadata": {
        "id": "QPb2QGY6Ol7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Faker;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvFJ7iRBG1Xt",
        "outputId": "5053f61a-4e56-4707-89f2-dca0006bcae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Faker\n",
            "  Downloading Faker-24.2.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from Faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->Faker) (1.16.0)\n",
            "Installing collected packages: Faker\n",
            "Successfully installed Faker-24.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the desired locale to English - India"
      ],
      "metadata": {
        "id": "Wchgh1S1PxVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from faker import Faker\n",
        "\n",
        "\n",
        "faker = Faker('en_IN')\n",
        "\n",
        "\n",
        "paragraphs = []\n",
        "for _ in range(500):\n",
        "    # Generate synthetic personal data\n",
        "    name = faker.name()\n",
        "    age = faker.random_int(min=18, max=100)\n",
        "    phone_number = faker.phone_number()\n",
        "    address = faker.address()\n",
        "\n",
        "\n",
        "    paragraph = f\"Hello there! I hope you're having a good day. Could you please tell me your name? \" \\\n",
        "                f\"Sure, my name is {name}. \" \\\n",
        "                f\"Great! And how old are you? \" \\\n",
        "                f\"I'm {age} years old. \" \\\n",
        "                f\"Thanks! What's your phone number, in case we need to reach you? \" \\\n",
        "                f\"My phone number is {phone_number}. \" \\\n",
        "                f\"Perfect! And where do you live? \" \\\n",
        "                f\"My address is {address}.\"\n",
        "\n",
        "    # Define patterns to search for entities\n",
        "    patterns = {\n",
        "        \"NAME OF PATIENT\": name,\n",
        "        \"AGE\": str(age),\n",
        "        \"NUMBER\": phone_number,\n",
        "        \"ADDRESS\": address\n",
        "    }\n",
        "\n",
        "    # Find positions of entities using regular expressions\n",
        "    entity_positions = {}\n",
        "    for entity, value in patterns.items():\n",
        "        matches = re.finditer(re.escape(value), paragraph)\n",
        "        entity_positions[entity] = [(match.start(), match.end()) for match in matches][0]\n",
        "\n",
        "    # Annotation format\n",
        "    annotation = {\n",
        "        \"paragraph\": paragraph,\n",
        "        \"entities\": [\n",
        "            {\"start\": start, \"end\": end, \"label\": entity}\n",
        "            for entity, (start, end) in entity_positions.items()\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Add annotation to list\n",
        "    paragraphs.append(annotation)\n",
        "\n",
        "\n",
        "with open(\"generated_paragraphs.json\", \"w\") as json_file:\n",
        "    json.dump(paragraphs, json_file, indent=2)\n",
        "\n",
        "print(\"Annotations saved to generated_paragraphs.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0ZFo3qBG1L0",
        "outputId": "fe20faab-a1d5-4b48-a699-5eca1165ba85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annotations saved to generated_paragraphs.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "from faker import Faker\n",
        "from faker.providers import person, date_time\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "faker = Faker('en_IN')\n",
        "\n",
        "\n",
        "faker.add_provider(person)\n",
        "faker.add_provider(date_time)\n",
        "\n",
        "\n",
        "def generate_paragraph():\n",
        "    # Generate synthetic personal data\n",
        "    name = faker.name()\n",
        "    gender = faker.random_element(elements=('Male', 'Female', 'Other'))\n",
        "    age = faker.random_int(min=18, max=100)\n",
        "    weight = faker.random_int(min=40, max=150)\n",
        "    phone_number = faker.phone_number()\n",
        "    address = faker.address()\n",
        "    admission_date = faker.date_this_year()\n",
        "\n",
        "\n",
        "    paragraph = f\"Good morning! Welcome to our clinic. \" \\\n",
        "                f\"Could you please provide me with your name? Sure, my name is {name}. \" \\\n",
        "                f\"I am {gender.lower()}, {age} years old. My weight is {weight} kg. \" \\\n",
        "                f\"My phone number is {phone_number}. My address is {address}. \" \\\n",
        "                f\"I visited the clinic today, {admission_date.strftime('%B %d, %Y')}.\"\n",
        "\n",
        "\n",
        "    patterns = {\n",
        "        \"NAME OF PATIENT\": name,\n",
        "        \"AGE\": str(age),\n",
        "        \"GENDER\": gender,\n",
        "        \"WEIGHT\": str(weight),\n",
        "        \"NUMBER\": phone_number,\n",
        "        \"ADDRESS\": address,\n",
        "        \"DATE OF ADMISSION\": admission_date.strftime('%B %d, %Y')\n",
        "    }\n",
        "\n",
        "    # Find positions of entities using regular expressions\n",
        "    entity_positions = {}\n",
        "    for entity, value in patterns.items():\n",
        "        matches = re.finditer(re.escape(value), paragraph)\n",
        "        matches_list = list(matches)\n",
        "        if matches_list:\n",
        "            entity_positions[entity] = [(match.start(), match.end()) for match in matches_list][0]\n",
        "\n",
        "    # Return the paragraph and its annotation\n",
        "    return {\n",
        "        \"paragraph\": paragraph,\n",
        "        \"annotation\": {\n",
        "            \"entities\": [\n",
        "                [start, end, entity]\n",
        "                for entity, (start, end) in entity_positions.items()\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "\n",
        "paragraphs_with_annotations = [generate_paragraph() for _ in range(500)]\n",
        "\n",
        "# Define the directory path\n",
        "directory_path = '/content/drive/MyDrive/dataset_generator/'\n",
        "\n",
        "\n",
        "if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)\n",
        "\n",
        "\n",
        "file_path = os.path.join(directory_path, 'generated_paragraphs.json')\n",
        "with open(file_path, \"w\") as file:\n",
        "    json.dump(paragraphs_with_annotations, file, indent=2)\n",
        "\n",
        "print(f\"500 paragraphs with annotations saved to {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obY74Tm6HPL6",
        "outputId": "e1f5f06d-9e3b-4259-eb1f-e104bbce6641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "500 paragraphs with annotations saved to /content/drive/MyDrive/dataset_generator/generated_paragraphs.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "from faker import Faker\n",
        "from faker.providers import person, date_time, address\n",
        "import string\n",
        "\n",
        "\n",
        "faker = Faker('en_IN')\n",
        "\n",
        "\n",
        "faker.add_provider(person)\n",
        "faker.add_provider(date_time)\n",
        "faker.add_provider(address)\n",
        "\n",
        "\n",
        "def generate_paragraph():\n",
        "    # Generate synthetic personal data\n",
        "    name = faker.name()\n",
        "    gender = faker.random_element(elements=('Male', 'Female', 'Other'))\n",
        "    age = faker.random_int(min=18, max=100)\n",
        "    weight = faker.random_int(min=40, max=150)\n",
        "    admission_date = faker.date_this_year()\n",
        "    address_line = faker.address().replace('\\n', ', ')\n",
        "\n",
        "\n",
        "    paragraph = f\"Hi there! I hope you're doing well today. Can I ask for your name? \" \\\n",
        "                f\"Of course! My name is {name}. \" \\\n",
        "                f\"And what is your gender? \" \\\n",
        "                f\"I am {gender.lower()}. \" \\\n",
        "                f\"Thank you! And how old are you? \" \\\n",
        "                f\"I'm {age} years old. \" \\\n",
        "                f\"Great! Also, can you provide your weight? \" \\\n",
        "                f\"My weight is {weight} kg. \" \\\n",
        "                f\"Perfect! Just one more thing, do you know today's date? \" \\\n",
        "                f\"Yes, today is {admission_date.strftime('%B %d, %Y')}. \" \\\n",
        "                f\"By the way, where do you live? \" \\\n",
        "                f\"I live at {address_line}.\"\n",
        "\n",
        "\n",
        "    paragraph = paragraph.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Define patterns to search for entities\n",
        "    patterns = {\n",
        "        \"NAME OF PATIENT\": name,\n",
        "        \"GENDER\": gender,\n",
        "        \"AGE\": str(age),\n",
        "        \"WEIGHT\": str(weight),\n",
        "        \"DATE OF ADMISSION\": admission_date.strftime('%B %d, %Y'),\n",
        "        \"ADDRESS\": address_line\n",
        "    }\n",
        "\n",
        "    # Find positions of entities using regular expressions\n",
        "    entity_positions = {}\n",
        "    for entity, value in patterns.items():\n",
        "        matches = re.finditer(re.escape(value), paragraph)\n",
        "        matches_list = list(matches)\n",
        "        if matches_list:\n",
        "            entity_positions[entity] = [(match.start(), match.end()) for match in matches_list][0]\n",
        "\n",
        "    # Return the paragraph and its annotation\n",
        "    return {\n",
        "        \"paragraph\": paragraph,\n",
        "        \"annotation\": {\n",
        "            \"entities\": [\n",
        "                [start, end, entity]\n",
        "                for entity, (start, end) in entity_positions.items()\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "\n",
        "paragraphs_with_annotations = [generate_paragraph() for _ in range(500)]\n",
        "\n",
        "\n",
        "directory_path = '/content/drive/MyDrive/dataset_generator/'\n",
        "\n",
        "if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)\n",
        "\n",
        "file_path = os.path.join(directory_path, 'generated_paragraphs_with_annotations.json')\n",
        "with open(file_path, \"w\") as file:\n",
        "    json.dump(paragraphs_with_annotations, file, indent=2)\n",
        "\n",
        "print(f\"500 paragraphs with annotations saved to {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG-fQ6ziIe3q",
        "outputId": "0a2613a0-0027-4bb3-b4dd-9b0756bd3ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500 paragraphs with annotations saved to /content/drive/MyDrive/dataset_generator/generated_paragraphs_with_annotations.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combined all the 3 json files and shuffled them"
      ],
      "metadata": {
        "id": "R7XD77ndPj23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Define the paths to the JSON files\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/dataset_generator/generated_paragraphs.json',\n",
        "    '/content/drive/MyDrive/dataset_generator/generated_paragraphs2.json',\n",
        "    '/content/drive/MyDrive/dataset_generator/generated_paragraphs3.json'\n",
        "]\n",
        "\n",
        "# Load the content of each JSON file\n",
        "merged_content = []\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = json.load(file)\n",
        "        merged_content.extend(content)\n",
        "\n",
        "# Shuffle the merged content while keeping paragraphs and annotations aligned\n",
        "random.shuffle(merged_content)\n",
        "\n",
        "# Define the path for the new merged and shuffled JSON file\n",
        "merged_file_path = '/content/drive/MyDrive/dataset_generator/merged_paragraphs_shuffled.json'\n",
        "\n",
        "# Save the shuffled merged content to a new JSON file\n",
        "with open(merged_file_path, 'w') as file:\n",
        "    json.dump(merged_content, file, indent=2)\n",
        "\n",
        "print(f\"Shuffled content saved to {merged_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-pLSELQJ2NY",
        "outputId": "4b0cbc26-96fb-426f-91f8-77101c939056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffled content saved to /content/drive/MyDrive/dataset_generator/merged_paragraphs_shuffled.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividing the merged_paragraphs_shuffled.json into testing and training data\n",
        "*80% of the data for training, 20% for testing*"
      ],
      "metadata": {
        "id": "0wRYqg_vQzQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Load the shuffled merged content\n",
        "merged_file_path = '/content/drive/MyDrive/dataset_generator/merged_paragraphs_shuffled.json'\n",
        "with open(merged_file_path, 'r') as file:\n",
        "    merged_content = json.load(file)\n",
        "\n",
        "# Define the percentage split between training and testing data\n",
        "train_percent = 0.8  # 80% of the data for training, 20% for testing\n",
        "\n",
        "# Calculate the number of examples for training and testing\n",
        "num_examples = len(merged_content)\n",
        "num_train = int(train_percent * num_examples)\n",
        "num_test = num_examples - num_train\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data = merged_content[:num_train]\n",
        "test_data = merged_content[num_train:]\n",
        "\n",
        "# Define paths for the training and testing JSON files\n",
        "train_file_path = '/content/drive/MyDrive/dataset_generator/train_data.json'\n",
        "test_file_path = '/content/drive/MyDrive/dataset_generator/test_data.json'\n",
        "\n",
        "# Save the training data to a JSON file\n",
        "with open(train_file_path, 'w') as file:\n",
        "    json.dump(train_data, file, indent=2)\n",
        "\n",
        "# Save the testing data to a JSON file\n",
        "with open(test_file_path, 'w') as file:\n",
        "    json.dump(test_data, file, indent=2)\n",
        "\n",
        "print(f\"Training data saved to {train_file_path}\")\n",
        "print(f\"Testing data saved to {test_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3oBDCmUJ9Wg",
        "outputId": "eff09e86-dcf7-4a11-efb4-75ed77c78b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data saved to /content/drive/MyDrive/dataset_generator/train_data.json\n",
            "Testing data saved to /content/drive/MyDrive/dataset_generator/test_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x3HzLWUWRFEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "i-CT9iVrKDzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySabpjw8KIF_",
        "outputId": "5438a488-7da1-40eb-e18f-97205d4e27d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/content/drive/MyDrive/dataset_generator/train_data.json', 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "b_cmkzXXKJff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]['annotation']['entities']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIRkvHm7KX1H",
        "outputId": "16d73461-8208-43a3-ee47-c1d66739f76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[89, 99, 'NAME OF PATIENT'],\n",
              " [174, 176, 'AGE'],\n",
              " [243, 245, 'WEIGHT'],\n",
              " [320, 337, 'DATE OF ADMISSION']]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "\n",
        "nlp = spacy.blank(\"en\") # load a new spacy model\n",
        "doc_bin = DocBin()"
      ],
      "metadata": {
        "id": "F9MQntK-KdMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.util import filter_spans\n",
        "from spacy.tokens import DocBin\n",
        "import spacy\n",
        "import json\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Load the dataset from the JSON file\n",
        "with open('/content/drive/MyDrive/dataset_generator/train_data.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Initialize a DocBin to store the processed documents\n",
        "doc_bin = DocBin()\n",
        "\n",
        "# Iterate through each training example\n",
        "for training_example in data:\n",
        "    text = training_example['paragraph']\n",
        "\n",
        "    # Check if the 'annotation' key exists in the current example\n",
        "    if 'annotation' in training_example:\n",
        "        labels = training_example['annotation']['entities']\n",
        "\n",
        "        # Create a Doc object from the text\n",
        "        doc = nlp.make_doc(text)\n",
        "\n",
        "        # Initialize a list to store the entities\n",
        "        ents = []\n",
        "\n",
        "        # Iterate through each entity label in the example\n",
        "        for start, end, label in labels:\n",
        "            # Create a span for the entity and add it to the list of entities\n",
        "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "            if span is None:\n",
        "                print(\"Skipping entity\")\n",
        "            else:\n",
        "                ents.append(span)\n",
        "\n",
        "        # Filter the spans to ensure there are no overlaps\n",
        "        filtered_ents = filter_spans(ents)\n",
        "\n",
        "        # Assign the filtered entities to the Doc object\n",
        "        doc.ents = filtered_ents\n",
        "\n",
        "        # Add the processed document to the DocBin\n",
        "        doc_bin.add(doc)\n",
        "\n",
        "# Define the file path where you want to save the spaCy binary file\n",
        "file_path = '/content/drive/MyDrive/dataset_generator/train.spacy'\n",
        "\n",
        "# Save the processed documents to the specified file path\n",
        "doc_bin.to_disk(file_path)\n",
        "\n",
        "print(f\"Processed documents saved to {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "718HWoslKhDH",
        "outputId": "69901d1c-97a7-4491-8e95-5c396a59a592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed documents saved to /content/drive/MyDrive/dataset_generator/train.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config /content/drive/MyDrive/dataset_generator/base_config.cfg /content/drive/MyDrive/dataset_generator/config.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzSsyESfLpw_",
        "outputId": "51013ff9-7d86-49dc-dd2c-e1033d8f0d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "/content/drive/MyDrive/dataset_generator/config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Download and install the large English model\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "\n",
        "# Load the model\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woy6KYJbMl9p",
        "outputId": "872c9b7a-7781-4a9e-8915-989de8c4270d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train /content/drive/MyDrive/dataset_generator/config.cfg --output /content/drive/MyDrive/dataset_generator --paths.train /content/drive/MyDrive/dataset_generator/train.spacy --paths.dev /content/drive/MyDrive/dataset_generator/train.spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z18FETtUMyB4",
        "outputId": "9765f8aa-55d7-4a18-9c80-be131d32fd3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/content/drive/MyDrive/dataset_generator\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     34.92    0.00    0.00    0.00    0.00\n",
            "  0     200         25.37    844.63   99.45   99.32   99.57    0.99\n",
            "  0     400          4.45     33.34   99.45   99.32   99.57    0.99\n",
            "  0     600          6.12     21.78   99.77   99.65   99.90    1.00\n",
            "  1     800          4.32     15.79   99.77   99.65   99.90    1.00\n",
            "  2    1000         39.81     93.56   99.45   99.32   99.57    0.99\n",
            "  2    1200         17.57     41.52   99.82   99.75   99.90    1.00\n",
            "  3    1400         22.27     27.16   99.77   99.65   99.90    1.00\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "/content/drive/MyDrive/dataset_generator/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_ner = spacy.load(\"/content/drive/MyDrive/dataset_generator/model-best\")"
      ],
      "metadata": {
        "id": "qricN6eON_yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp_ner(\"Hello there! I hope you're having a good day. Could you please tell me your name? Sure, my name is Tanya Bhargava. Great! And how old are you? I'm 36 years old. Thanks! What's your phone number, in case we need to reach you? My phone number is 8992902281. Perfect! And where do you live? My address is 01/871, Shankar Road, Bhiwani 054861.\")\n",
        "\n",
        "colors = {\n",
        "    \"NAME OF PATIENT\": \"#F67DE3\",\n",
        "    \"AGE\": \"#7DF6D9\",\n",
        "    \"WEIGHT\": \"#a6e22d\",\n",
        "    \"NUMBER\": \"#FF5733\",\n",
        "    \"ADDRESS\": \"#3498db\",\n",
        "    \"DATE OF ADMISSION\": \"#f39c12\"\n",
        "}\n",
        "options = {\"colors\": colors}\n",
        "\n",
        "spacy.displacy.render(doc, style=\"ent\", options= options, jupyter=True)\n",
        "mapped = []\n",
        "\n",
        "for ent in doc.ents:\n",
        "    mapped.append({\n",
        "        \"text\": ent.text,\n",
        "        \"label\": ent.label_\n",
        "    })\n",
        "\n",
        "print(mapped)"
      ],
      "metadata": {
        "id": "uO_GYyeJOBzq",
        "outputId": "71d1e31d-15bc-48fa-b316-8285c4d7604f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hello there! I hope you're having a good day. Could you please tell me your name? Sure, my name is \n",
              "<mark class=\"entity\" style=\"background: #F67DE3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tanya Bhargava\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME OF PATIENT</span>\n",
              "</mark>\n",
              ". Great! And how old are you? I'm \n",
              "<mark class=\"entity\" style=\"background: #7DF6D9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    36\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">AGE</span>\n",
              "</mark>\n",
              " years old. Thanks! What's your phone number, in case we need to reach you? My phone number is \n",
              "<mark class=\"entity\" style=\"background: #FF5733; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    8992902281\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NUMBER</span>\n",
              "</mark>\n",
              ". Perfect! And where do you live? My address is \n",
              "<mark class=\"entity\" style=\"background: #3498db; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    01/871, Shankar Road, Bhiwani 054861\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ADDRESS</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'text': 'Tanya Bhargava', 'label': 'NAME OF PATIENT'}, {'text': '36', 'label': 'AGE'}, {'text': '8992902281', 'label': 'NUMBER'}, {'text': '01/871, Shankar Road, Bhiwani 054861', 'label': 'ADDRESS'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pickle\n",
        "\n",
        "# Load your trained model\n",
        "nlp_ner = spacy.load(\"/content/drive/MyDrive/dataset_generator/model-best\")\n",
        "\n",
        "# Dump the model to a pickle file\n",
        "with open(\"/content/drive/MyDrive/dataset_generator/model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(nlp_ner, f)"
      ],
      "metadata": {
        "id": "qDDy3G97OR1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the epoch data\n",
        "epoch_data = [\n",
        "    {\"ENTS_P\": 99.32, \"ENTS_R\": 99.57, \"ENTS_F\": 99.45},\n",
        "    {\"ENTS_P\": 99.65, \"ENTS_R\": 99.90, \"ENTS_F\": 99.77},\n",
        "    {\"ENTS_P\": 99.75, \"ENTS_R\": 99.90, \"ENTS_F\": 99.82},\n",
        "]\n",
        "\n",
        "# Calculate accuracy (using F1-score as an approximation)\n",
        "accuracy = sum(epoch[\"ENTS_F\"] for epoch in epoch_data) / len(epoch_data)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ6Dt6Wr-X19",
        "outputId": "51000b1f-bcf8-4beb-9e61-70103ea267fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 99.67999999999999\n"
          ]
        }
      ]
    }
  ]
}